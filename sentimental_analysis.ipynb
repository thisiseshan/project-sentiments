{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimental-analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddCUEK0FsFVT",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csGQKot6sEBn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am92fyz8sPwq",
        "colab_type": "text"
      },
      "source": [
        "###Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1shm9z9tsRoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmDBBQduo37b",
        "colab_type": "text"
      },
      "source": [
        "###Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOqBhTQwn85R",
        "colab_type": "code",
        "outputId": "c676aa57-a9f8-40f1-fa86-46b464e3e123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !unzip archive.zip\n",
        "# %cd archive\n",
        "#%cd data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/archive/data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpZshfuoqKjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_counter = Counter()\n",
        "negative_counter = Counter()\n",
        "total_counts = Counter()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtLULdSOCeGU",
        "colab_type": "code",
        "outputId": "1191fb1e-2b43-461c-a470-448b7dfe3168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mQtoicuXD9",
        "colab_type": "text"
      },
      "source": [
        "##Curating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6bnAQkzsB2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_review_and_label(i):\n",
        "  print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
        "  \n",
        "g = open('reviews.txt','r')\n",
        "reviews = list(map(lambda x:x[:-1], g.readlines()))\n",
        "g.close()\n",
        "\n",
        "g = open('labels.txt','r')\n",
        "labels = list(map(lambda x:x[:-1], g.readlines()))\n",
        "g.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyjurZePuwm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_review_and_label(1)\n",
        "print_review_and_label(2)\n",
        "print_review_and_label(3)\n",
        "print_review_and_label(4)\n",
        "print_review_and_label(5)\n",
        "print_review_and_label(6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFCI8Fq3pJ7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(reviews)):\n",
        "  if(labels[i] == 'positive'):\n",
        "    for word in reviews[i].split(\" \"):\n",
        "      positive_counter[word] += 1\n",
        "      total_counts[word] += 1\n",
        "  else:\n",
        "    for word in reviews[i].split(\" \"):\n",
        "      negative_counter[word] += 1\n",
        "      total_counts[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnOOzr59xmm7",
        "colab_type": "text"
      },
      "source": [
        "##Create Input/Output Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwkHRYOerxFw",
        "colab_type": "code",
        "outputId": "a7f5f8b6-0186-46ea-ad68-353c60ea6169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = set(total_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmRXag3xzhD",
        "colab_type": "code",
        "outputId": "2bfba896-00a6-401a-acb2-6974b4855a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "layer_0 = np.zeros((1,vocab_size))\n",
        "layer_0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGiFBKMcyBi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {}\n",
        "\n",
        "for i,word in enumerate(vocab):\n",
        "  word2index[word] = i\n",
        "\n",
        "\n",
        "word2index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-huUtNsszFHf",
        "colab_type": "code",
        "outputId": "5fa92499-87c7-48ff-9790-6a1ef7c102e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "layer_0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 74074)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD2JzUXBKwPn",
        "colab_type": "code",
        "outputId": "b3b490ba-ee22-4bd1-e2f0-20e563603cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzMhD1Nz9sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_input_layer(review):\n",
        "  \n",
        "  global layer_0\n",
        "  \n",
        "  #clear out all previous state, reset the layer to all 0s\n",
        "  layer_0 *= 0\n",
        "  for word in review.split(\" \"):\n",
        "    layer_0[0][word2index[word]] += 1\n",
        "    \n",
        "update_input_layer(reviews[0])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0dk8iEDRe5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ATnUR2R27Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_target_for_label(label):\n",
        "  if(label == 'positive'):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBP9V68D300w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,100):\n",
        "  print(get_target_for_label(labels[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dZc-32CMJ2X",
        "colab_type": "text"
      },
      "source": [
        "##Create Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8JCRXI5374J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1VI6h2TMNo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentNeuralNet:\n",
        "  \n",
        "  \n",
        "  def __init__(self,input_dim,hidden_dim,output_dim):\n",
        "    super(neuralnet, self).__init__()\n",
        "    \n",
        "    # Assign a seed to our random number generator to ensure we get reproducable results during development\n",
        "    np.random.seed(1)\n",
        "    \n",
        "    # process the reviews and their associated labels so that everything is ready for training\n",
        "    self.pre_process_data(reviews, labels)\n",
        "    \n",
        "    # Build the network to have the number of hidden nodes and the learning rate that were passed into this initializer. Make the same number of input nodes as\n",
        "    # there are vocabulary words and create a single output node.\n",
        "    self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
        "   \n",
        "  \n",
        "    \n",
        "  def pre_process_data(self, reviews, labels):\n",
        "    \n",
        "    review_vocab = set()            # set does not contain dupliactes\n",
        "    for review in reviews:\n",
        "      for word in reviews.split(\" \"):\n",
        "        review_vocab.add(word)\n",
        "    self.review_vocab = list(review_vocab)       # coverting to list for easy iteration\n",
        "    \n",
        "    \n",
        "    label_vocab = set()\n",
        "    for label i labels:\n",
        "      label_vocab.add(label)\n",
        "    self.label_vocab = list(label_vocab)\n",
        "    \n",
        "   \n",
        "    self.review_vocab_size = len(review_vocab)\n",
        "    self.review_vocab_size = len(label_vocab)\n",
        "\n",
        "    self.word2index = {}\n",
        "    for i, word in enumerate(self.review_vocab):       # enumerate allows us to loop over something and have an automatic counter.\n",
        "      self.word2index[word] = i\n",
        "\n",
        "    self.word2index = {}\n",
        "    for i, label in enumerate(self.label_vocab):       # enumerate allows us to loop over something and have an automatic counter.\n",
        "      self.word2index[word] = i\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "  def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "    # Set values\n",
        "    self.input_nodes = review_vocab_size\n",
        "    self.hidden_nodes = hidden_nodes\n",
        "    self.output_nodes = review_vocab_size\n",
        "      \n",
        "    # Set learning rate\n",
        "    self.learning_rate = learning_rate\n",
        "    \n",
        "    # Initialize weights\n",
        "    \n",
        "    # Weights between input layer and hidden layer\n",
        "\n",
        "    self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
        "    \n",
        "    # Weights between hidden layer and output layer\n",
        "    self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) \n",
        "    # syntax  numpy.random.normal(loc=0.0, scale=1.0, size=None)\n",
        " \n",
        "    # The input layer, 2 fim matrix, 1 x hidden inputs\n",
        "    self.layer_1 = np.zeros((1, hidden_nodes))\n",
        "                            \n",
        "  def sigmoid(self,x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "                 \n",
        "  def sigmoid_output_2_derivative(self,output):\n",
        "    return output * (1 - output)                        \n",
        "      \n",
        "                            \n",
        "  def get_target_for_label(label):\n",
        "    if(label == 'positive'):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "  def train(self, trainig_reviews_raw, training_labels):\n",
        "    training_reviews = list()\n",
        "    for review in training_reviews_raw:\n",
        "      indices = set()\n",
        "            for word in review.split(\" \"):\n",
        "                if(word in self.word2index.keys()):\n",
        "                    indices.add(self.word2index[word])\n",
        "            training_reviews.append(list(indices))\n",
        "\n",
        "    # Number of reviews = Number of Labels\n",
        "    assert(len(training_reviews) == len(training_labels))\n",
        "                            \n",
        "                            \n",
        "    # Keep track of correct             \n",
        "    correct_so_far = 0\n",
        "             \n",
        "    # Start time\n",
        "    start = time.time()\n",
        "                            \n",
        "    # Loop over all reviews and labels, run forward and backward pass\n",
        "    for i in range(len(training_reviews)):\n",
        "      \n",
        "      # Get the next review and label\n",
        "      review = training_reviews[i]\n",
        "      label = training_labels[i]\n",
        "      \n",
        "      ##FORWARD PASS\n",
        "      \n",
        "      # Hidden Layer\n",
        "      # Add in weights for non-zero items\n",
        "      self.layer *= 0\n",
        "      for index in review:\n",
        "        self.layer_1 += self.weights_0_1[index]   \n",
        "      \n",
        "      # Output Layer\n",
        "      layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
        "      \n",
        "      \n",
        "      ##BACKWARD PASS\n",
        "      \n",
        "      # Output error\n",
        "      layer_2_error = layer_2 - self.get_target_for_label(label)  # Output layer error is the difference between desired target and actual output.\n",
        "      layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)  # output * (output - 1)\n",
        "      \n",
        "      # Backpropagation Error\n",
        "      layer_1_error = layer_2_delta.dot(self.weights_1_2.T)  # errors propagated to the hidden layer\n",
        "      layer_1_delta = layer_1_error   # hidden layer gradients - no nonlinearity so it's the same as the error\n",
        "      \n",
        "      \n",
        "      ##UPDATING WEIGHTS\n",
        "      \n",
        "      self.weights_1_2 -= self.layer_1T.dot(layer_2_delta) * self.learning_rate   # update input-to-hidden weights with gradient descent step\n",
        "      \n",
        "      # Keep track of correct predictions\n",
        "      if(layer_2 >= 0.5 and label ='positive'):\n",
        "        correct_so_far += 1\n",
        "      elif(layer <= 0.5 and label = 'negative'):\n",
        "        correct_so_far -= 1\n",
        "        \n",
        "      # Prinitng out prediction accuracy, speed throughout the training process\n",
        "      elapsed_time = float(time.time() - start)\n",
        "      sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
        "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
        "      \n",
        "  def test(self, testing_reviews, testing_labels):\n",
        "    # Calculate accuracy from testing_labels and testing_reviews\n",
        "    \n",
        "    # keep track of correct predictions\n",
        "    correct = 0\n",
        "    \n",
        "    # time how many predictions we make per second\n",
        "    start = time.time()\n",
        "    \n",
        "    # Loop through each review and call run to predict corresponding label\n",
        "    for i in range(le(testing_reviews)):\n",
        "      pred = self.run(testing_reviews[i])\n",
        "      if(pred == testing_labels[i]):\n",
        "        correct+=1\n",
        "      \n",
        "      \n",
        "      if(i % 2500 == 0):\n",
        "                print(\"\")\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}